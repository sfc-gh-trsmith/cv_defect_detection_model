{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Notebook (Required to run this from the local directory where the dataset has been downloaded)\n",
    "\n",
    "1. This notebook is a *PRE-REQUISITE* for the 1_Distributed_Model_Training_Snowflake_Notebooks.ipynb notebook. \n",
    "2. The solution leverages an open source dataset that is available [here](https://github.com/Charmve/Surface-Defect-Detection/tree/master/DeepPCB/PCBData) for downloading. Be sure to review and comply with the licensing terms and usage guidelines of that repository owner before utilizing the dataset. \n",
    "\n",
    "#### After cloning the DeepPCB dataset to your local, use this notebook to extract the images, convert into base64 code and load the contents along with the label information into Snowflake Stage\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries. Remember to refer to the env.yaml for the complete list of libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE = 'PCB_CV'\n",
    "SCHEMA = 'PUBLIC'\n",
    "ROLE = 'PCB_CV_ROLE'\n",
    "WAREHOUSE = 'PCB_CV_WH'\n",
    "COMPUTEPOOL = 'PCB_CV_COMPUTEPOOL'\n",
    "STAGE = 'PCB_CV_DEEP_PCB_DATASET_STAGE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish connection to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.config(\"connection_name\", \"demo\").create()\n",
    "session.use_database(DATABASE)\n",
    "session.use_schema(SCHEMA)\n",
    "session.use_role(ROLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CURRENT_USER()='TRSMITH')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = session.sql(\"select current_user()\")\n",
    "d.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the folder structure in the original repository to identify the appropriate .jpg and .txt files in the PCBData folder and upload them to the specified Snowflake stages. The script uses the os module to traverse the directory and the Snowflake Python connector to handle file uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/PCBData/group44000/44000_not/44000022.txt\n",
      "Uploaded file: ../data/PCBData/group44000/44000/44000022_test.jpg to @PCB_CV_DEEP_PCB_DATASET_STAGE/images/train\n",
      "Uploaded file: ../data/PCBData/group44000/44000_not/44000022.txt to @PCB_CV_DEEP_PCB_DATASET_STAGE/labels/train\n",
      "../data/PCBData/group44000/44000_not/44000023.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def upload_to_snowflake(stage, file_path, file_type):\n",
    "    \"\"\"\n",
    "    Upload a file to the specified Snowflake stage.\n",
    "\n",
    "    Args:\n",
    "        stage (str): Snowflake stage path (e.g., @data_stage/images/train).\n",
    "        file_path (str): Path to the file to upload.\n",
    "        file_type (str): Type of file being uploaded \n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        session.file.put(f\"file://{file_path}\", stage,auto_compress= False)\n",
    "        #session.sql(f\"PUT 'file://{file_path}' {stage}\")\n",
    "        print(f\"Uploaded file: {file_path} to {stage}\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload  file: {file_path}. Error: {str(e)}\")\n",
    "        return 1\n",
    "\n",
    "def process_and_upload_files(base_dir, image_stage, label_stage):\n",
    "    \"\"\"\n",
    "    Process the directory structure and upload _test.jpg files and their corresponding .txt files to Snowflake.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Path to the base directory containing PCB data.\n",
    "        image_stage (str): Snowflake stage path for images.\n",
    "        label_stage (str): Snowflake stage path for labels.\n",
    "    \"\"\"\n",
    "    for group_folder in os.listdir(base_dir):\n",
    "        group_path = os.path.join(base_dir, group_folder)\n",
    "        if not os.path.isdir(group_path):\n",
    "            continue\n",
    "\n",
    "        for sub_folder in os.listdir(group_path):\n",
    "            sub_folder_path = os.path.join(group_path, sub_folder)\n",
    "            \n",
    "\n",
    "            if not os.path.isdir(sub_folder_path):\n",
    "                continue\n",
    "\n",
    "            if sub_folder.endswith(\"_not\"):\n",
    "                continue\n",
    "\n",
    "            folder_not = os.path.join(group_path, sub_folder + \"_not\")\n",
    "\n",
    "            if not os.path.exists(folder_not):\n",
    "                continue\n",
    "\n",
    "            for file_name in os.listdir(sub_folder_path):\n",
    "                if file_name.endswith(\"_test.jpg\"):\n",
    "                    \n",
    "                    # Full path of the .jpg file\n",
    "                    jpg_file_path = os.path.join(sub_folder_path, file_name)\n",
    "                    \n",
    "                    # Corresponding .txt file path\n",
    "                    txt_file_name = os.path.splitext(file_name.replace(\"_test\", \"\"))[0] + \".txt\"\n",
    "                    txt_file_path = os.path.join(folder_not, txt_file_name)\n",
    "                    print(txt_file_path)\n",
    "                    if os.path.exists(txt_file_path):\n",
    "                        # Upload the .jpg file to the images stage\n",
    "                        rc = upload_to_snowflake(image_stage_path,jpg_file_path, \"image\")\n",
    "                        if rc == 1:\n",
    "                            printf(\"Upload Failed.\")\n",
    "                            break\n",
    "\n",
    "                        # Upload the .txt file to the labels stage\n",
    "                        rc = upload_to_snowflake(label_stage_path,txt_file_path, \"label\")\n",
    "                        if rc == 1:\n",
    "                            printf(\"Upload Failed.\")\n",
    "                            break\n",
    "\n",
    "\n",
    "\n",
    "# Define local directory where the PCB dataset was downloaded\n",
    "base_directory = \"../data/PCBData\" #ensure this notebook is in the same directory as the PCBData folder\n",
    "image_stage_path = f\"@{STAGE}/images/train\"\n",
    "label_stage_path = f\"@{STAGE}/labels/train\"\n",
    "\n",
    "# Process and upload files\n",
    "process_and_upload_files(base_directory, image_stage_path, label_stage_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Labels \n",
    "Download label files from the Snowflake stage, parse each label text file to extract label data, and then save this data into a DataFrame which is written to a Snowflake table called LABELS_TRAIN.\n",
    "\n",
    "After successful data upload to the Snowflake Internal Stage, follow the below steps. The label contains the Xmin,Ymin,Xmax and Ymax coorrdinates of the defects and the class of defect that will be used to train this supervised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import pandas as pd\n",
    "stage_train_path = \"@DATA_STAGE/labels/train\"\n",
    "local_train_dir = \"/tmp/labels/\"\n",
    "\n",
    "\n",
    "# Ensure local directories exist\n",
    "os.makedirs(local_train_dir, exist_ok=True)\n",
    "\n",
    "#train_files = session.file.get(stage_train_path, \"/tmp/labels/\")\n",
    "# Download .txt files from the train stage\n",
    "train_files = session.file.get(stage_train_path, local_train_dir, pattern=\".*\\.txt\")\n",
    "\n",
    "def download_files_from_stage(stage_path, local_dir):\n",
    "    \n",
    "    files = session.file.get(stage_path, local_dir)\n",
    "    \n",
    "    # Check if files were downloaded successfully\n",
    "    if files:\n",
    "        for file in files:\n",
    "            \n",
    "            local_file_path = os.path.join(local_dir, os.path.basename(file.file))\n",
    "            \n",
    "    else:\n",
    "        print(f\"No .txt files were downloaded from {stage_path}.\")\n",
    "\n",
    "# Download .txt files from the train stage\n",
    "download_files_from_stage(stage_train_path, local_train_dir)\n",
    "path_annot=\"/tmp/labels/\"\n",
    "\n",
    "# Initialize a list to hold all the data  \n",
    "data = []  \n",
    "  \n",
    "# Walking through the directory to get all label files  \n",
    "for path, subdirs, files in os.walk(path_annot):  \n",
    "    for name in files:  \n",
    "        if name.endswith('.txt'):  # Filter to include only .txt files  \n",
    "            full_path = os.path.join(path, name)  \n",
    "            with open(full_path, 'r') as file:  \n",
    "                for line in file:  \n",
    "                    parts = line.strip().split()  \n",
    "                    if len(parts) == 5:\n",
    "                        xmin, ymin, xmax, ymax,class_id = parts  \n",
    "                        data.append({  \n",
    "                            \"filename\": name.replace('.txt', ''), \n",
    "                            \"xmin\": float(xmin),  \n",
    "                            \"ymin\": float(ymin),  \n",
    "                            \"xmax\": float(xmax),  \n",
    "                            \"ymax\": float(ymax) ,\n",
    "                            \"class\": int(class_id)\n",
    "                        })  \n",
    "  \n",
    "# Create a DataFrame  \n",
    "trainlabels_df = pd.DataFrame(data)\n",
    "train_labels = session.create_dataframe(trainlabels_df)\n",
    "\n",
    "train_labels.write.save_as_table(\"LABELS_TRAIN\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Images \n",
    " Downloads image files from a Snowflake stage, encode them in Base64, merges them with label data that was processed in the last step, and inserts the combined information into a Snowflake table named train_images_labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainlabels_df columns: Index(['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'class'], dtype='object')\n",
      "train_images_df columns: Index(['Filename', 'image_data'], dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "train_images_stage_path = \"@DATA_STAGE/images/\"\n",
    "\n",
    "\n",
    "local_train_images_dir = \"/tmp/images/train\"\n",
    "\n",
    "\n",
    "os.makedirs(local_train_images_dir, exist_ok=True)\n",
    "\n",
    "# Function to download images from Snowflake stage and convert them to Base64\n",
    "def download_images_from_stage(stage_path, local_dir):\n",
    "    # Use session.file.get() to fetch images\n",
    "    files = session.file.get(stage_path, local_dir)\n",
    "    \n",
    "    # List to hold the image data\n",
    "    images_data = []\n",
    "    \n",
    "    if files:\n",
    "        for file in files:\n",
    "            filename = os.path.basename(file.file)            \n",
    "            # Read the image and convert to Base64\n",
    "            with open(os.path.join(local_dir, filename), \"rb\") as image_file:\n",
    "                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "                images_data.append({\"Filename\": filename.replace(\".jpg\", \"\"), \"image_data\": encoded_string})\n",
    "    else:\n",
    "        print(f\"No image files were downloaded from {stage_path}.\")\n",
    "    \n",
    "    return images_data\n",
    "\n",
    "# Download the train and validation images and convert to Base64\n",
    "train_images_base64_data = download_images_from_stage(train_images_stage_path, local_train_images_dir)\n",
    "\n",
    "# Convert to DataFrames for inserting into separate tables\n",
    "train_images_df = pd.DataFrame(train_images_base64_data)\n",
    "\n",
    "print(\"trainlabels_df columns:\", trainlabels_df.columns)\n",
    "print(\"train_images_df columns:\", train_images_df.columns)\n",
    "\n",
    "if 'filename' in trainlabels_df.columns:\n",
    "    trainlabels_df.rename(columns={'filename': 'Filename'}, inplace=True)\n",
    "# Update labels DataFrame to include `_test.jpg` suffix for matching\n",
    "trainlabels_df['Filename'] = trainlabels_df['Filename'] + \"_test\"\n",
    "\n",
    "merged_train_df = pd.merge(trainlabels_df, train_images_df, how='inner', on='Filename')\n",
    "\n",
    "\n",
    "\n",
    "# Create the Snowflake table schema to accommodate both image data and label information\n",
    "create_train_table_query = \"\"\"\n",
    "CREATE OR REPLACE TABLE train_images_labels (\n",
    "    Filename varchar,\n",
    "    image_data VARCHAR,\n",
    "    class INT,\n",
    "    xmin FLOAT,\n",
    "    ymin FLOAT,\n",
    "    xmax FLOAT,\n",
    "    ymax FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "# Execute the table creation query\n",
    "session.sql(create_train_table_query).collect()\n",
    "\n",
    "# Function to insert merged image and label data into the specified table\n",
    "def insert_images_and_labels_into_table(merged_df, table_name):\n",
    "    for index, row in merged_df.iterrows():\n",
    "        filename = row['Filename'].replace('_test', '')\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} (Filename, image_data, class, xmin, ymin, xmax, ymax)\n",
    "        VALUES ('{filename}', '{row['image_data']}', '{row['class']}', {row['xmin']}, {row['ymin']}, {row['xmax']}, {row['ymax']})\n",
    "        \"\"\"\n",
    "        print(\"\")\n",
    "        session.sql(insert_query).collect()\n",
    "\n",
    "# Insert merged data into the train table\n",
    "insert_images_and_labels_into_table(merged_train_df, \"train_images_labels\")\n",
    "\n",
    "print(\"Merged image and label data inserted into table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=session.table(\"TRAIN_IMAGES_LABELS\").to_pandas()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "session.write_pandas(train_df,\"training_data\", auto_create_table=True, overwrite=True, quote_identifiers=False)\n",
    "\n",
    "session.write_pandas(test_df,\"test_data\", auto_create_table=True, overwrite=True,quote_identifiers=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Setup and Data Preparation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
